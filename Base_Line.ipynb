{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, layers, models\n",
    "# from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.layers import Input, TimeDistributed, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'VIDEO_LENGTH': 50,\n",
    "    'IMG_SIZE': 128,\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE':4,\n",
    "    'SEED': 714\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) ## Seed 고정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    for _ in range(CFG['VIDEO_LENGTH']):\n",
    "        _, img = cap.read()\n",
    "        img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "        img = img / 255\n",
    "        frames.append(img)\n",
    "    result = np.array(frames)[..., [2,1,0]]\n",
    "    return result\n",
    "\n",
    "class FrameGenerator:\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __call__(self):\n",
    "        for tp in self.dataframe.itertuples():\n",
    "            video_frames = frames_from_video_file(tp.video_path)\n",
    "            label = tp.label\n",
    "            label = to_categorical(label, 13)\n",
    "\n",
    "            yield video_frames, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "train, val, _, _ =train_test_split(df, df['label'], test_size=0.2, stratify=df['label'], random_state=CFG['SEED'])\n",
    "weights = compute_class_weight(class_weight='balanced', classes=np.unique(train.label), y=train.label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 23:52:41.271678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-09 23:52:41.353845: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sangho/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2023-03-09 23:52:41.353866: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-09 23:52:41.354521: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 23:52:41.386534: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-03-09 23:52:41.874503: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames:(4, 50, 128, 128, 3)\n",
      "Shape of training labels:(4, 13)\n",
      "Shape of validation set of frames:(4, 50, 128, 128, 3)\n",
      "Shape of validation labels:(4, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 23:52:42.405034: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "## Model에 주입하기 위한 dataset 객체 생성\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(13), dtype=tf.int16))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(train),\n",
    "                                          output_signature=output_signature)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(val),\n",
    "                                        output_signature=output_signature)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "train_ds = train_ds.batch(CFG['BATCH_SIZE'])\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.batch(CFG['BATCH_SIZE'])\n",
    "\n",
    "train_frames, train_labels = next(iter(train_ds))\n",
    "print(f'Shape of training set of frames:{train_frames.shape}')\n",
    "print(f'Shape of training labels:{train_labels.shape}')\n",
    "\n",
    "val_frames, val_labels = next(iter(val_ds))\n",
    "print(f'Shape of validation set of frames:{val_frames.shape}')\n",
    "print(f'Shape of validation labels:{val_labels.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 50, 126, 126, 32)  896       \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 50, 126, 126, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 50, 63, 63, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 50, 62, 62, 64)    8256      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 62, 62, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 50, 31, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 50, 30, 30, 128)   32896     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 50, 30, 30, 128)   512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 50, 15, 15, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 50, 14, 14, 256)   131328    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 50, 14, 14, 256)   1024      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 50, 7, 7, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 50, 6, 6, 512)     524800    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 50, 6, 6, 512)     2048      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 50, 3, 3, 512)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 50, 2, 2, 1024)    2098176   \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 50, 2, 2, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13)                13325     \n",
      "=================================================================\n",
      "Total params: 2,817,741\n",
      "Trainable params: 2,813,709\n",
      "Non-trainable params: 4,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, TimeDistributed, Dense\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv3D(32, (1, 3, 3), activation='relu', input_shape=(50, 128, 128, 3)))\n",
    "model.add(layers.TimeDistributed(layers.BatchNormalization()))\n",
    "model.add(layers.MaxPooling3D((1, 2, 2)))\n",
    "\n",
    "model.add(layers.Conv3D(64, (1, 2, 2), activation='relu'))\n",
    "model.add(layers.TimeDistributed(layers.BatchNormalization()))\n",
    "model.add(layers.MaxPooling3D((1, 2, 2)))\n",
    "\n",
    "model.add(layers.Conv3D(128, (1, 2, 2), activation='relu'))\n",
    "model.add(layers.TimeDistributed(layers.BatchNormalization()))\n",
    "model.add(layers.MaxPooling3D((1, 2, 2)))\n",
    "\n",
    "model.add(layers.Conv3D(256, (1, 2, 2), activation='relu'))\n",
    "model.add(layers.TimeDistributed(layers.BatchNormalization()))\n",
    "model.add(layers.MaxPooling3D((1, 2, 2)))\n",
    "\n",
    "model.add(layers.Conv3D(512, (1, 2, 2), activation='relu'))\n",
    "model.add(layers.TimeDistributed(layers.BatchNormalization()))\n",
    "model.add(layers.MaxPooling3D((1, 2, 2)))\n",
    "\n",
    "model.add(layers.Conv3D(1024, (1, 2, 2), activation='relu'))\n",
    "model.add(layers.TimeDistributed(layers.BatchNormalization()))\n",
    "model.add(layers.GlobalAveragePooling3D())\n",
    "\n",
    "model.add(layers.Dense(13, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(CFG['LEARNING_RATE']),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 23:55:05.950048: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 406425600 exceeds 10% of free system memory.\n",
      "2023-03-09 23:55:06.085843: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 406425600 exceeds 10% of free system memory.\n",
      "2023-03-09 23:55:08.661660: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 406425600 exceeds 10% of free system memory.\n",
      "2023-03-09 23:55:10.412706: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 406425600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 6s 6s/step - loss: 0.8404 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 23:55:10.781798: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 406425600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    105/Unknown - 492s 5s/step - loss: 3.4573 - accuracy: 0.0762"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_10377/1035146544.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mEarlyStopping\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'val_accuracy'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'max'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrestore_best_weights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m history = model.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=[es],\n\u001B[0;32m----> 3\u001B[0;31m                     class_weight={i:weights[i] for i in range(len(weights))})\n\u001B[0m",
      "\u001B[0;32m~/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1182\u001B[0m                 _r=1):\n\u001B[1;32m   1183\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1184\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1185\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1186\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    883\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    884\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 885\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    886\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    887\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    915\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 917\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    918\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    919\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3038\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   3039\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 3040\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   3041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3042\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1962\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1963\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1964\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1965\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1966\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    594\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    595\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 596\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    597\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    598\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/.pyenv/versions/3.7.16/envs/python37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)\n",
    "history = model.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=[es],\n",
    "                    class_weight={i:weights[i] for i in range(len(weights))})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
